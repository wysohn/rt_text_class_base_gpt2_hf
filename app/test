#! /usr/bin/env python

# The function to execute the testing.
import io
import sys
import os
import traceback

from algorithm import Model, Preprocessor
from algorithm import data_loader

from algorithm.misc.config import *


def test():
    try:
        # Load configuarations and data
        hyper_parameters = data_loader.get_hyperparameters(
            MODEL_HYPERPARAMETERS_FILE_PATH)
        data = data_loader.get_data(INPUT_DATA_FOLDER_PATH)
        data_schema = data_loader.get_data_schema(
            INPUT_DATA_CONFIG_FOLDER_PATH)

        # Prepare preprocessor
        preprocessor = Preprocessor({
            'tokenizer_path': os.path.join(PRETRAINED_MODEL_FOLDER_PATH, 'gpt2_tokenizer')
        }, data_schema, {**hyper_parameters, **{'sample_size': 0}})  # testing uses all samples

        # load preprocessor saved data
        preprocessor.load_weights(
            MODEL_ARTIFACT_FOLDER_PATH, **PREPROCESSOR_SAVE_CONFIG)

        # Prepare model
        model = Model({
            'model_path': os.path.join(PRETRAINED_MODEL_FOLDER_PATH, 'gpt2_pretrained'),
            'num_labels': preprocessor.num_classes(),
        }, hyper_parameters)

        model.load_weights(MODEL_ARTIFACT_FOLDER_PATH,
                           **MODEL_SAVE_CONFIG)

        # Preprocess raw test data
        if EVALUATE_FILE:
            path = os.path.join(
                INPUT_DATA_TEST_KEY_FOLDER_PATH, EVALUATE_FILE)
            data = data_loader.attach_test_labels(
                data, path, data_schema.col_label_key())
            print(data)

        processed_data_dict = preprocessor.transform(
            data, EVALUATE_FILE is not None)

        # # evaluate model
        if EVALUATE_FILE:
            print("Evaluating model against {}".format(EVALUATE_FILE))
            print("loss: {}".format(model.evaluate(processed_data_dict['X'],
                                                   processed_data_dict['y'])))

        # Make predictions
        predictions = model.predict(processed_data_dict['X'])
        predictions = preprocessor.post_processing(predictions)

        # Append ids to predictions
        predictions.insert(0, data_schema.col_id_key(),
                           processed_data_dict['ids'])

        # Save predictions
        data_loader.save_predictions(
            predictions, OUTPUT_TESTING_FOLDER_PATH)

    except Exception as e:
        print("error!")
        # Write out an error file. This will be returned as the failureReason to the client.
        trc = traceback.format_exc()
        with open(OUTPUT_ERROR_TEST_FAILURE_FILE_PATH, 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during testing: ' +
              str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    test()
